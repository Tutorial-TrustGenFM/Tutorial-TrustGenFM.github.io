<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Socially Responsible and Trustworthy Generative Foundation Models: Principles, Challenges, and Practices</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <img src="images/cikm_logo.jpg" alt="CIKM 2025" class="cikm-logo">
                <!-- <span>TrustGenFM</span> -->
            </div>
            <ul class="nav-menu">
                <li><a href="#overview">Overview</a></li>
                <li><a href="#outline">Outline</a></li>
                <li><a href="#presenters">Presenters</a></li>
            </ul>
            <div class="nav-toggle">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero">
        <div class="hero-container">
            <div class="hero-content">
                <h1 class="hero-title">Trust and Responsibility in Generative Foundation Models</h1>
                <p class="hero-subtitle">CIKM 2025 Tutorial - A Comprehensive Overview of Building Socially Responsible AI Systems</p>
                <div class="hero-stats">
                    <div class="stat">
                        <i class="fas fa-clock"></i>
                        <span>3 Hours</span>
                        <small>Half-day Session</small>
                    </div>
                    <div class="stat">
                        <i class="fas fa-users"></i>
                        <span>Interactive</span>
                        <small>Hands-on Exercises</small>
                    </div>
                    <div class="stat">
                        <i class="fas fa-graduation-cap"></i>
                        <span>Expert-led</span>
                        <small>Industry & Academia</small>
                    </div>
                </div>
                <div class="hero-buttons">
                    <a href="#outline" class="btn btn-primary">View Tutorial Outline</a>
                    <a href="#presenters" class="btn btn-secondary">Meet the Presenters</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Overview Section -->
    <section id="overview" class="section">
        <div class="container">
            <div class="section-header">
                <h2>Overview</h2>
            </div>
            <div class="overview-text">
                <p>Generative foundation models (GenFMs), such as large language and multimodal models, are transforming information access, retrieval, and knowledge systems. However, their deployment raises critical concerns around social responsibility, including fairness, bias mitigation, environmental impact, misinformation, and safety.</p>
                
                <p>This tutorial provides a comprehensive overview of recent research and best practices at the intersection of GenFMs and responsible AI. We introduce foundational concepts, evaluation metrics, and mitigation strategies, with case studies across various domains (e.g., text, vision, code).</p>
                
                <p>The tutorial is designed for researchers and practitioners interested in building or auditing socially responsible GenFMs. We highlight open challenges and propose future research directions relevant to the CIKM community.</p>
            </div>
        </div>
    </section>

    <!-- Tutorial Outline Section -->
    <section id="outline" class="section bg-light">
        <div class="container">
            <div class="section-header">
                <h2>Tutorial Outline</h2>
                <p>Six comprehensive modules covering theory, practice, and policy perspectives</p>
            </div>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-play"></i>
                    </div>
                    <div class="timeline-content">
                        <div class="timeline-time">20 minutes</div>
                        <h3>The Dual Nature of GenFMs and the Need for Responsibility</h3>
                        <p>Understanding how GenFMs serve as assistants and simulators, and why responsible behavior is critical. We'll explore societal risks including misinformation, bias, and privacy concerns.</p>
                        <div class="timeline-tags">
                            <span class="tag">Foundation</span>
                            <span class="tag">Risk Assessment</span>
                        </div>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-search"></i>
                    </div>
                    <div class="timeline-content">
                        <div class="timeline-time">45 minutes</div>
                        <h3>Understanding Social Responsibility: Taxonomy and Case Studies</h3>
                        <p>A structured framework covering six key dimensions: Safety, Privacy, Robustness, Truthfulness, Fairness, and Machine Ethics. Includes hands-on exercises with diagnostic tools.</p>
                        <div class="responsibility-dimensions">
                            <div class="dimension">
                                <i class="fas fa-shield-alt"></i>
                                <span>Safety</span>
                            </div>
                            <div class="dimension">
                                <i class="fas fa-user-secret"></i>
                                <span>Privacy</span>
                            </div>
                            <div class="dimension">
                                <i class="fas fa-mountain"></i>
                                <span>Robustness</span>
                            </div>
                            <div class="dimension">
                                <i class="fas fa-search"></i>
                                <span>Truthfulness</span>
                            </div>
                            <div class="dimension">
                                <i class="fas fa-balance-scale"></i>
                                <span>Fairness</span>
                            </div>
                            <div class="dimension">
                                <i class="fas fa-brain"></i>
                                <span>Machine Ethics</span>
                            </div>
                        </div>
                        <div class="timeline-tags">
                            <span class="tag">Hands-on</span>
                            <span class="tag">Case Studies</span>
                            <span class="tag">Tools</span>
                        </div>
                    </div>
                </div>

                <div class="timeline-break">
                    <i class="fas fa-coffee"></i>
                    <span>15-minute Break</span>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-chart-bar"></i>
                    </div>
                    <div class="timeline-content">
                        <div class="timeline-time">20 minutes</div>
                        <h3>Evaluation and Benchmarks</h3>
                        <p>Introduction to evaluation frameworks including BBQ, TruthfulQA, HarmBench, and TrustLLM. Hands-on experience with tools like OpenAI Evals and AI Fairness 360.</p>
                        <div class="timeline-tags">
                            <span class="tag">Benchmarks</span>
                            <span class="tag">Metrics</span>
                            <span class="tag">Evaluation</span>
                        </div>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-wrench"></i>
                    </div>
                    <div class="timeline-content">
                        <div class="timeline-time">20 minutes</div>
                        <h3>Enhancement for Responsible GenFMs</h3>
                        <p>Practical mitigation strategies including data filtering, prompt steering, model fine-tuning (RLHF), and post-processing techniques like RAG and detoxification.</p>
                        <div class="timeline-tags">
                            <span class="tag">Mitigation</span>
                            <span class="tag">RLHF</span>
                            <span class="tag">RAG</span>
                        </div>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-gavel"></i>
                    </div>
                    <div class="timeline-content">
                        <div class="timeline-time">20 minutes</div>
                        <h3>Governance and Policy Perspectives</h3>
                        <p>Overview of regulatory frameworks (EU AI Act, NIST AI RMF), industry initiatives, and community standards for responsible AI deployment.</p>
                        <div class="timeline-tags">
                            <span class="tag">Policy</span>
                            <span class="tag">Governance</span>
                            <span class="tag">Standards</span>
                        </div>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-comments"></i>
                    </div>
                    <div class="timeline-content">
                        <div class="timeline-time">25 minutes</div>
                        <h3>Open Challenges and Community Discussion</h3>
                        <p>Interactive discussion on adaptive evaluation, dual effects of alignment, and advanced AI risks. Collaborative exploration of future research directions.</p>
                        <div class="timeline-tags">
                            <span class="tag">Discussion</span>
                            <span class="tag">Future Research</span>
                            <span class="tag">Interactive</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Presenters Section -->
    <section id="presenters" class="section">
        <div class="container">
            <div class="section-header">
                <h2>Presenters</h2>
            </div>
            <div class="presenters-row">
                <div class="presenter-item">
                    <div class="presenter-photo">
                        <img src="images/yuehuang.jpeg" alt="Yue Huang">
                    </div>
                    <div class="presenter-info">
                        <h4>Yue Huang</h4>
                        <p class="presenter-title">Ph.D. Student, Computer Science and Engineering, University of Notre Dame</p>
                        <div class="presenter-bio">
                            <p>Yue Huang is a Ph.D. student in Computer Science and Engineering at the University of Notre Dame. He earned his B.S. in Computer Science from Sichuan University. His research investigates the trustworthiness and social responsibility of foundation models. Yue has published extensively at premier venues including NeurIPS, ICLR, ICML, ACL, EMNLP, NAACL, CVPR, and IJCAI. His work has been highlighted by the U.S. Department of Homeland Security and recognized with the Microsoft Accelerating Foundation Models Research Award and the KAUST AI Rising Star Award (2025). He has delivered invited talks on "Trustworthiness in Large Language Models" and "Socially Responsible Generative Foundation Models" at UIUC, USC, UVA, IBM Research, and other institutions.</p>
                        </div>
                    </div>
                </div>
                
                <div class="presenter-item">
                    <div class="presenter-photo">
                        <img src="images/canyuchen.jpg" alt="Canyu Chen">
                    </div>
                    <div class="presenter-info">
                        <h4>Canyu Chen</h4>
                        <p class="presenter-title">Ph.D. Student, Northwestern University</p>
                        <div class="presenter-bio">
                            <p>Canyu Chen is a Ph.D. student at Northwestern University. He focuses on truthful, safe and responsible Large Language Models with the applications in social computing and healthcare. He has started and led an initiative "LLMs Meet Misinformation" (<a href="https://llm-misinformation.github.io" target="_blank">https://llm-misinformation.github.io</a>), aiming to combat misinformation in the age of LLMs. He has publications in top-tier conferences including ICLR, NeurIPS, EMNLP, EACL, and WWW. He won multiple awards such as Sigma Xi Student Research Award 2024, the Didactic Paper Award in the workshop ICBINB@NeurIPS 2023, Spotlight Research Award in the AGI Leap Summit 2024. He is a co-organizer of the Workshop Reasoning and Planning for Large Language Models at ICLR 2025.</p>
                        </div>
                    </div>
                </div>
                
                <div class="presenter-item">
                    <div class="presenter-photo">
                        <img src="images/chenglu.png" alt="Lu Cheng">
                    </div>
                    <div class="presenter-info">
                        <h4>Lu Cheng</h4>
                        <p class="presenter-title">Assistant Professor, Computer Science, University of Illinois Chicago</p>
                        <div class="presenter-bio">
                            <p>Lu Cheng is an assistant professor in Computer Science at the University of Illinois Chicago. Her research interests are responsible and reliable AI, causal machine learning, and AI for social good. She is the recipient of the PAKDD Best Paper Award, Google Research Scholar Award, Amazon Research Award, Cisco Research Faculty award, AAAI New Faculty Highlights, 2022 INNS Doctoral Dissertation Award (runner-up), 2021 ASU Engineering Dean's Dissertation Award, SDM Best Poster Award, IBM Ph.D. Social Good Fellowship, Visa Research Scholarship, among others. She co-authors two books: "Causal Inference and Machine Learning (Chinese)" and "Socially Responsible AI: Theories and Practices".</p>
                        </div>
                    </div>
                </div>
                
                <div class="presenter-item">
                    <div class="presenter-photo">
                        <img src="images/Bhavya.jpeg" alt="Bhavya Kailkhura">
                    </div>
                    <div class="presenter-info">
                        <h4>Bhavya Kailkhura</h4>
                        <p class="presenter-title">Staff Scientist, Lawrence Livermore National Laboratory</p>
                        <div class="presenter-bio">
                            <p>Bhavya Kailkhura a Staff Scientist and a council member of the Data Science Institute (DSI) at LLNL. He leads efforts on AI safety, efficiency, and their applications to science and national security. His work has earned several awards, including the All-University Doctoral Prize (Syracuse Uni., 2017), the LLNL Early and Mid Career Recognition Program Award (2024), and the best paper awards at ICLR SRML, AAAI CoLoRAI, and others. He is an IEEE Senior Member and served as Associate Editor for ACM JATS (2023) and Frontiers in Big Data and AI (2021). He has held roles such as panelist, program chair, and organizer for workshops and conferences including ICASSP, AAAI, and GlobalSIP.</p>
                        </div>
                    </div>
                </div>
                
                <div class="presenter-item">
                    <div class="presenter-photo">
                        <img src="images/nitesh.webp" alt="Nitesh Chawla">
                    </div>
                    <div class="presenter-info">
                        <h4>Nitesh Chawla</h4>
                        <p class="presenter-title">Frank M. Freimann Professor, University of Notre Dame</p>
                        <div class="presenter-bio">
                            <p>Nitesh Chawla is the Frank M. Freimann Professor of Computer Science and Engineering at the University of Notre Dame. He is the Founding Director of the Lucy Family Institute for Data and Society. He is an expert in artificial intelligence, data science, and network science, and is motivated by the question of how technology can advance the common good through interdisciplinary research. He is the recipient of 2015 IEEE CIS Outstanding Early Career Award; the IBM Watson Faculty Award; the IBM Big Data and Analytics Faculty Award; and the 1st Source Bank Technology Commercialization Award. He was recognized with the Rodney F. Ganey Award and Michiana 40 under 40 honor. He is a Fellow of both ACM and IEEE.</p>
                        </div>
                    </div>
                </div>
                
                <div class="presenter-item">
                    <div class="presenter-photo">
                        <img src="images/xiangliangzhang.png" alt="Xiangliang Zhang">
                    </div>
                    <div class="presenter-info">
                        <h4>Xiangliang Zhang</h4>
                        <p class="presenter-title">Leonard C. Bettex Collegiate Professor, University of Notre Dame</p>
                        <div class="presenter-bio">
                            <p>Xiangliang Zhang is a Leonard C. Bettex Collegiate Professor in the Department of Computer Science and Engineering, University of Notre Dame. She was an Associate Professor in Computer Science at the KAUST. Her main research interests and experiences are in machine learning and data mining. She has published more than 270 refereed papers in leading international conferences and journals. She serves as associate editor of IEEE Transactions on Dependable and Secure Computing, Information Sciences, and International Journal of Intelligent Systems, and regularly serves as area chair or on the (senior) program committee of IJCAI, SIGKDD, NeurIPS, AAAI, ICML, and WSDM.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <script src="script.js"></script>
</body>
</html> 